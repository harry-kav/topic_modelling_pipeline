import ast
import datetime
import logging
import os
import random
from itertools import product
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import pyLDAvis
import pyLDAvis.gensim_models as gensimvis
from gensim import corpora
from gensim.models import LdaModel, CoherenceModel
from gensim.test.utils import datapath
from sklearn.model_selection import train_test_split
from tqdm import tqdm
from joblib import Parallel, delayed
from wordcloud import WordCloud, STOPWORDS
import matplotlib.colors as mcolors
from collections import Counter

logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

FILE_NAME = 'ngrams_processed_tripadvisor_hotel_reviews.csv' #change to ngrams_processed filename (generated by eda.py)
RANDOM_STATE = 42
DATA_SOURCE = f'Data/{FILE_NAME}'
random.seed(RANDOM_STATE)
np.random.seed(RANDOM_STATE)
tqdm.pandas()

OUTPUT_DIR = 'Results'
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

TIME_NOW_STR = str(datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
OUTPUT_FOLDER = f"Run_{TIME_NOW_STR}"
OUTPUT_PATH = f"{OUTPUT_DIR}/{OUTPUT_FOLDER}/"

def plot_difference(mdiff, title="LDA", annotation=None):
    #helper function to plot difference between models

    fig, ax = plt.subplots(figsize=(18,14))
    data = ax.imshow(mdiff, cmap='RdBu_r', origin='lower')
    plt.title(title)
    plt.colorbar(data)

    if annotation is not None:
        ax.set_xticks(np.arange(len(annotation)))
        ax.set_yticks(np.arange(len(annotation)))
        ax.set_xticklabels(annotation, rotation=90)
        ax.set_yticklabels(annotation)
    
    plt.savefig(f'{OUTPUT_PATH}Topic Difference {title}.png')

def train_lda_model(num_topics, passes, iterations, train_corpus, heldout_corpus, dictionary, tokens):
    #function for model training

    print(f'Training model with {num_topics} topics, {passes} passes, {iterations} iterations')

    model = LdaModel(
        corpus=train_corpus,
        id2word=dictionary,
        num_topics=num_topics,
        alpha='auto',
        eta='auto',
        passes=passes,
        iterations=iterations,
        random_state=42,
        eval_every=0
    )

    coherence = CoherenceModel(
        model=model,
        texts=tokens,
        dictionary=dictionary,
        coherence='c_v', #you can choose between c_v and u_mass for evaluation
        #coherence='u_mass',
        processes=1, #needed if using c_v  
    ).get_coherence()

    perplexity = model.log_perplexity(heldout_corpus)

    return {
        'num_topics' : num_topics,
        'passes' : passes,
        'iterations' : iterations,
        'alpha' : model.alpha,
        'eta': model.eta,
        'coherence' : coherence,
        'perplexity' : perplexity,
        'model' : model
    }

def format_topics_sentences(ldamodel, corpus, texts):
    # Init output
    sent_topics_df = pd.DataFrame()

    # Get main topic in each document
    for i, row_list in enumerate(ldamodel[corpus]):
        row = row_list[0] if ldamodel.per_word_topics else row_list            
        # print(row)
        row = sorted(row, key=lambda x: (x[1]), reverse=True)
        # Get the Dominant topic, Perc Contribution and Keywords for each document
        for j, (topic_num, prop_topic) in enumerate(row):
            if j == 0:  # => dominant topic
                wp = ldamodel.show_topic(topic_num)
                topic_keywords = ", ".join([word for word, prop in wp])
                sent_topics_df.loc[len(sent_topics_df)] = [
                    int(topic_num),
                    round(prop_topic, 4),
                    topic_keywords
                ]
            else:
                break
    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']

    # Add original text to the end of the output
    contents = pd.Series(texts)
    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)
    return(sent_topics_df)

def main():
    
    if not os.path.exists(OUTPUT_PATH):
        os.makedirs(OUTPUT_PATH)

    df = pd.read_csv(DATA_SOURCE)
    token_col = 'tokens'
    bigram_col = 'bigrams'
    text_col = 'Review'

    include_bigrams = True #set false if you just want the pure ngrams - should work for large datasets

    df[token_col] = df[token_col].progress_apply(ast.literal_eval)

    if include_bigrams:
        df[bigram_col] = df[bigram_col].progress_apply(ast.literal_eval)
        df[bigram_col] = df[bigram_col].progress_apply(lambda bigram_list: ['_'.join(tup) for tup in bigram_list])
        df[token_col] = df[token_col] + df[bigram_col]

    print('Tokens read successfully')

    #build gensim dictionary and corpus
    dictionary = corpora.Dictionary(df[token_col])
    print('Dict length: ', len(dictionary))
    dictionary.filter_extremes(no_below=5, no_above=0.5) # can tweak these values but these are a good baseline
    dictionary.compactify()
    print('Dict length after filtering: ', len(dictionary))
    corpus = [dictionary.doc2bow(tokens) for tokens in df[token_col]]
    print('Corpus created successfully')

    #withhold 5% of corpus for measuring log_perplexity on unseen data
    train_corpus, heldout_corpus = train_test_split(corpus, test_size=0.05, random_state=RANDOM_STATE)

    #hyperparameter grid - exponential increase in combos with each addition
    param_grid = {
        'num_topics' : [5, 10, 15, 20],#, 25, 30], #number of topics the model will attempt to generate
        'passes' : [1, 3, 5],#, 7, 10], # number of times the model will see the full training corpus during training - high number of passes will take a long time on large datasets
        'iterations' : [10, 15, 20],# 30, 50, 75, 100], #how many times we refine topic-word distribution per pass
    }

    param_combinations = list(product(param_grid['num_topics'], param_grid['passes'], param_grid['iterations']))

    #train lda models, gridsearching across hyperparameters in param_combinations
    #training done in parallel to increase speed - set n_jobs to number of cores to use - go with -1 if running on Kaggle as that chooses all cores on the device - otherwise choose less cores than your device has if you need to use during training
    results = Parallel(n_jobs=-1, verbose=10, backend='loky')(
        delayed(train_lda_model)(
            num_topics = num_topics,
            passes = passes,
            iterations = iterations,
            train_corpus = train_corpus,
            heldout_corpus= heldout_corpus,
            dictionary = dictionary,
            tokens=df[token_col]
        )
    for num_topics, passes, iterations in tqdm(param_combinations)
    )

    #select best model and hyperparams
    best_result = max(results, key=lambda x: x['coherence'])
    best_model = best_result['model']
    best_params = {k: v for k, v in best_result.items() if k!= 'model'}

    #convert to df
    df_results = pd.DataFrame([{k: v for k, v in r.items() if k!='model'} for r in results])

    #show best params and top topic terms
    topic_terms = {}
    TOPN = 20
    print('Best model parameters: ', best_params)
    for i in tqdm(range(best_model.num_topics)):
        terms = [term for term, weight in best_model.show_topic(i, topn=TOPN)]
        print(f"Topic {i + 1}: {', '.join(terms)}")
        topic_terms[f'Topic {i + 1}'] = terms

    topic_words_df = pd.DataFrame.from_dict(topic_terms, orient='index').transpose()
    topic_words_df.to_csv(f'{OUTPUT_PATH}top_words_per_topic.csv', index=False)

    df_results.to_csv(f'{OUTPUT_PATH}results.csv', index=False)

    print('Getting doc topic probs')
    doc_topic_probs = []
    for bow in tqdm(corpus):
        #get probability for every document and topic in the model
        topic_dist = dict(best_model.get_document_topics(bow, minimum_probability=0))
        doc_topic_probs.append(topic_dist)

    print('Converting to df')
    topic_df = pd.DataFrame(doc_topic_probs).fillna(0)
    #rename columns
    topic_df.columns = [f'topic_{i}' for i in topic_df.columns]

    print('Combining with metadata')
    output_df = pd.concat(
        [df[[text_col]].reset_index(drop=True), topic_df], axis=1
    )

    print('Saving')
    output_df.to_csv(f'{OUTPUT_PATH}topic_dist_per_doc.csv', index=False)

    print('Visualising topics')
    vis_data = gensimvis.prepare(best_model, corpus, dictionary)

    #save to html - note that topic names here won't line up with the ones in the csv so you'll have to manually map them by looking at the top words per topic
    pyLDAvis.save_html(vis_data, f'{OUTPUT_PATH}lda_visualisation.html')

    print('Saving model')
    temp_file = datapath('model')
    best_model.save(temp_file)

    #this visualisation isn't that good and may save incorrectly
    print('Plotting difference')
    mdiff, annotation = best_model.diff(best_model, distance='jaccard', num_words=TOPN)
    plot_difference(mdiff, title='LDA Model (Jaccard Distance)', annotation=None)
    
    print("Script completed successfully")


    #bonus bits - not needed but could be interesting - will probably crash on these but all the important stuff already done and saved
    # TODO - make these work

    try:
        #dominant topic per doc, plus representative keywords/text
        df_topic_sents_keywords = format_topics_sentences(best_model, corpus, df[token_col])
        df_dominant_topic = df_topic_sents_keywords.reset_index()
        df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']
        print(df_dominant_topic.head(5))
        df_dominant_topic.to_csv(f'{OUTPUT_PATH}dominant_topic_per_doc.csv', index=False)


        #most representative sentence per topic

        # Display setting to show more characters in column
        pd.options.display.max_colwidth = 100

        sent_topics_sorteddf_mallet = pd.DataFrame()
        sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')

        for i, grp in sent_topics_outdf_grpd:
            sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, 
                                                    grp.sort_values(['Perc_Contribution'], ascending=False).head(1)], 
                                                    axis=0)

        # Reset Index    
        sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)

        # Format
        sent_topics_sorteddf_mallet.columns = ['Topic_Num', "Topic_Perc_Contrib", "Keywords", "Representative Text"]

        # Show
        print(sent_topics_sorteddf_mallet.head(10))
        sent_topics_sorteddf_mallet.to_csv(f'{OUTPUT_PATH}representative_sentences_per_topic.csv', index=False)


        #wordcounts/weighting per topic visualisation
        topics = best_model.show_topics(formatted=False)
        data_flat = [w for w_list in df[token_col] for w in w_list]
        counter = Counter(data_flat)

        out = []
        for i, topic in topics:
            for word, weight in topic:
                out.append([word, i , weight, counter[word]])

        df_word_importance = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        
        df_word_importance.to_csv(f'{OUTPUT_PATH}word_importance_per_topic.csv', index=False)
        # Plot Word Count and Weights of Topic Keywords
        fig, axes = plt.subplots(2, 2, figsize=(16,10), sharey=True, dpi=160)
        cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]
        for i, ax in enumerate(axes.flatten()):
            ax.bar(x='word', height="word_count", data=df_word_importance.loc[df_word_importance.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label='Word Count')
            ax_twin = ax.twinx()
            ax_twin.bar(x='word', height="importance", data=df_word_importance.loc[df_word_importance.topic_id==i, :], color=cols[i], width=0.2, label='Weights')
            ax.set_ylabel('Word Count', color=cols[i])
            ax_twin.set_ylim(0, 0.030); ax.set_ylim(0, 3500)
            ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=16)
            ax.tick_params(axis='y', left=False)
            ax.set_xticklabels(df_word_importance.loc[df_word_importance.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')
            ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')

        fig.tight_layout(w_pad=2)
        fig_title = 'Word Count and Importance of Topic Keywords'
        fig.suptitle(fig_title, fontsize=22, y=1.05)   
        plt.savefig(f'{OUTPUT_PATH}{fig_title}.png')

        print("Bonus bits done")
    except:
        print("Bonus bits failed (currently expected)")


if __name__ == '__main__':
    main()